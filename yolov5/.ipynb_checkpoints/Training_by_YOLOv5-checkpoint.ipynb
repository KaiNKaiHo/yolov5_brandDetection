{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fca70129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import yolov5\n",
    "import os\n",
    "import yaml\n",
    "from yolov5.models.yolo import Model\n",
    "from yolov5.utils.dataloaders import LoadImagesAndLabels, DataLoader\n",
    "from yolov5.utils.general import check_dataset, check_file, check_img_size, non_max_suppression, xywh2xyxy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62821f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5\\yolov5s.pt, cfg=, data=data.yaml, hyp=yolov5\\data\\hyps\\hyp.scratch-low.yaml, epochs=5, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5\\runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, weight=yolov5s.pt\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m YOLOv5 is out of date by 7 commits. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
      "YOLOv5  v7.0-162-gc3e4e94 Python-3.10.9 torch-1.12.1 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5  in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5\\runs\\train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=27\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\manhc\\yolov5\\requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     86304  models.yolo.Detect                      [27, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7092448 parameters, 7092448 gradients, 16.2 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5\\yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\manhc\\yolov5\\labels\\train... 809 images, 0 backgrounds, 0 corrupt: 100%|██████████| 809/809 [0\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  Cache directory C:\\Users\\manhc\\yolov5\\labels is not writeable: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\manhc\\\\yolov5\\\\labels\\\\train.cache.npy' -> 'C:\\\\Users\\\\manhc\\\\yolov5\\\\labels\\\\train.cache'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\manhc\\yolov5\\labels\\train... 809 images, 0 backgrounds, 0 corrupt: 100%|██████████| 809/809 [00:\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  Cache directory C:\\Users\\manhc\\yolov5\\labels is not writeable: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\manhc\\\\yolov5\\\\labels\\\\train.cache.npy' -> 'C:\\\\Users\\\\manhc\\\\yolov5\\\\labels\\\\train.cache'\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.23 anchors/target, 0.999 Best Possible Recall (BPR). Current anchors are a good fit to dataset \n",
      "Plotting labels to yolov5\\runs\\train\\exp3\\labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolov5\\runs\\train\\exp3\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/4         0G     0.1027    0.03119    0.08781         17        640: 100%|██████████| 51/51 [12:08<00:00, 14.\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 26/26 [02:20\n",
      "                   all        809       1261    0.00205      0.362     0.0106     0.0036\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/4         0G    0.07321    0.03026    0.08068         22        640: 100%|██████████| 51/51 [11:06<00:00, 13.\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 26/26 [02:24\n",
      "                   all        809       1261      0.133      0.107     0.0317     0.0127\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/4         0G      0.063    0.02697    0.07763         34        640: 100%|██████████| 51/51 [11:56<00:00, 14.\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 26/26 [02:25\n",
      "                   all        809       1261      0.676      0.041     0.0527     0.0258\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/4         0G    0.05558    0.02574    0.07505         40        640: 100%|██████████| 51/51 [10:39<00:00, 12.\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 26/26 [02:23\n",
      "                   all        809       1261      0.419     0.0974     0.0903     0.0412\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/4         0G    0.05143    0.02499    0.07309         23        640: 100%|██████████| 51/51 [09:54<00:00, 11.\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 26/26 [02:18\n",
      "                   all        809       1261      0.419      0.163      0.129     0.0673\n",
      "\n",
      "5 epochs completed in 1.129 hours.\n",
      "Optimizer stripped from yolov5\\runs\\train\\exp3\\weights\\last.pt, 14.5MB\n",
      "Optimizer stripped from yolov5\\runs\\train\\exp3\\weights\\best.pt, 14.5MB\n",
      "\n",
      "Validating yolov5\\runs\\train\\exp3\\weights\\best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7082944 parameters, 0 gradients, 16.0 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 26/26 [02:03\n",
      "                   all        809       1261      0.419      0.163      0.129     0.0672\n",
      "               Ferrari        809         30     0.0444     0.0333     0.0555     0.0242\n",
      "                  Ford        809         36      0.274      0.306       0.24      0.104\n",
      "                   Nbc        809         30       0.13      0.167      0.107     0.0673\n",
      "             Starbucks        809         48      0.168       0.75      0.571      0.295\n",
      "               RedBull        809         40          1          0     0.0218       0.01\n",
      "                  Mini        809         30      0.127      0.213      0.112      0.059\n",
      "                Unicef        809         50          1          0     0.0293     0.0173\n",
      "                 Yahoo        809         38      0.803      0.211      0.352      0.176\n",
      "                Sprite        809         64          0          0     0.0265     0.0123\n",
      "                Texaco        809         39          0          0     0.0323     0.0163\n",
      "                 Intel        809         38          1          0      0.041     0.0244\n",
      "              Cocacola        809         75      0.134      0.147      0.108     0.0412\n",
      "               Citroen        809         51          1          0     0.0539     0.0315\n",
      "              Heineken        809         42          0          0     0.0493     0.0323\n",
      "                 Apple        809         56          0          0     0.0207     0.0099\n",
      "                Google        809         34      0.524      0.486      0.478      0.314\n",
      "                 Fedex        809         34          0          0     0.0242     0.0121\n",
      "                 Pepsi        809         99      0.142      0.697      0.406      0.188\n",
      "                  Puma        809         40      0.458      0.025      0.172     0.0842\n",
      "                   DHL        809         33          1          0     0.0156    0.00761\n",
      "               Porsche        809         35        0.2      0.686      0.266      0.157\n",
      "                  Nike        809         47          1          0     0.0159    0.00686\n",
      "              Vodafone        809        100     0.0162       0.01       0.02    0.00773\n",
      "                   BMW        809         35      0.124      0.486      0.112     0.0544\n",
      "             McDonalds        809         34          1          0     0.0174    0.00792\n",
      "                    HP        809         33          1          0     0.0172    0.00759\n",
      "                Adidas        809         70       0.16      0.186      0.124     0.0469\n",
      "Results saved to \u001b[1myolov5\\runs\\train\\exp3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(weights='yolov5\\\\yolov5s.pt', cfg='', data='data.yaml', hyp={'lr0': 0.01, 'lrf': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.0, 'copy_paste': 0.0}, epochs=5, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket='', cache=None, image_weights=False, device='', multi_scale=False, single_cls=False, optimizer='SGD', sync_bn=False, workers=8, project='yolov5\\\\runs\\\\train', name='exp', exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias='latest', weight='yolov5s.pt', save_dir='yolov5\\\\runs\\\\train\\\\exp3')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_yaml_path = 'data.yaml'\n",
    "\n",
    "train.run(data=data_yaml_path, imgsz=640, epochs=5, weight='yolov5s.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7a43216-c316-4de6-b002-9648f14d0982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yolov5_model(cfg, weights):\n",
    "    with open(cfg, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "        config_path = data['model']\n",
    "    model = Model(config_path)\n",
    "    model.load_state_dict(torch.load(weights)['model'].state_dict())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da9d3390-10a7-4d94-8030-66f74fee1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_label_files(data_yaml):\n",
    "    with open(data_yaml, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "        train_dir = data['train']\n",
    "\n",
    "    '''image_dir = os.path.join(train_dir, 'images')\n",
    "    label_dir = os.path.join(train_dir, 'labels')\n",
    "\n",
    "    image_files = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.jpg')]\n",
    "    label_files = [os.path.join(label_dir, file) for file in os.listdir(label_dir) if file.endswith('.txt')]'''\n",
    "\n",
    "    #return image_files, label_files\n",
    "    return train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df31cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transform():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac609816-66f0-4808-a6f3-7b69fd9c8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(train_dir, model, epoch, img_size, batch_size):\n",
    "    #Xong\n",
    "    # Tạo mô hình YOLOv5\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device).eval()\n",
    "    \n",
    "    # Xong\n",
    "    # Chuẩn bị dữ liệu\n",
    "    dataset = LoadImagesAndLabels(train_dir, img_size=img_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    for batch in dataset:\n",
    "        imgs = batch\n",
    "        for img in imgs:\n",
    "            if img.shape[0] == 1:\n",
    "                img = img.squeeze(0)  # Nếu chiều kênh là 1, loại bỏ chiều kênh\n",
    "        \n",
    "            # Chuyển đổi tensor thành mảng numpy\n",
    "            img_np = img.permute(1, 2, 0).numpy()  # Chuyển đổi kích thước và định dạng kênh của ảnh\n",
    "        \n",
    "            # In ảnh ra\n",
    "            plt.imshow(img_np)\n",
    "            plt.show()\n",
    "    \n",
    "    '''# Kết quả dự đoán\n",
    "    results = []\n",
    "\n",
    "    # Huấn luyện và dự đoán\n",
    "    for e in range(epoch):\n",
    "        for batch_i, (imgs, _, targets, paths) in enumerate(dataloader):\n",
    "            # Kiểm tra kích thước ảnh\n",
    "            imgs = check_img_size(imgs) #ERROR\n",
    "            print(\"pass\")\n",
    "            break\n",
    "            # Chuẩn bị dữ liệu đầu vào\n",
    "            img_tensor = torch.stack(imgs).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(img_tensor)\n",
    "\n",
    "            # Tiếp tục quá trình huấn luyện theo yêu cầu của bạn...\n",
    "\n",
    "        # Lưu checkpoint sau mỗi epoch\n",
    "        torch.save(model.state_dict(), f\"checkpoint_epoch_{e}.pt\")\n",
    "\n",
    "    # Lưu kết quả vào thư mục \"result_img\" và tệp \"result.csv\"\n",
    "    result_folder = \"C:/Users/manhc/yolov5/result_test\"\n",
    "    os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "    result_img_folder = os.path.join(result_folder, 'result_img')\n",
    "    os.makedirs(result_img_folder, exist_ok=True)\n",
    "\n",
    "    result_csv_path = os.path.join(result_folder, 'result.csv')\n",
    "\n",
    "    with open(result_csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['image', 'x1', 'y1', 'x2', 'y2', 'confidence', 'class'])\n",
    "\n",
    "        for image, detections in results:\n",
    "            image_name = os.path.basename(image)\n",
    "            image_path = os.path.join(result_img_folder, image_name)\n",
    "            image.save(image_path)\n",
    "\n",
    "            for *bbox, conf, cls in detections:\n",
    "                x1, y1, x2, y2 = map(int, bbox)\n",
    "                class_name = dataset.names[int(cls)]\n",
    "                writer.writerow([image_name, x1, y1, x2, y2, conf, class_name])\n",
    "\n",
    "    print(f\"Kết quả đã được lưu vào thư mục {result_folder}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2a1895c8-12bc-42cc-b939-392e8575df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Đường dẫn đến các tệp cấu hình và trọng số\n",
    "    data_yaml = 'C:/Users/manhc/yolov5/yolov5/data.yaml'\n",
    "    cfg = 'C:/Users/manhc/yolov5/yolov5/config.yaml'\n",
    "    weights = 'C:/Users/manhc/yolov5/yolov5/yolov5s.pt'\n",
    "    \n",
    "    #Khởi tạo model \n",
    "    model = create_yolov5_model(cfg, weights)\n",
    "    \n",
    "    #Lấy đường dẫn đến tập huấn luyện\n",
    "    train_dir = get_image_and_label_files(data_yaml)\n",
    "    \n",
    "    # Huấn luyện và dự đoán\n",
    "    train_and_predict(train_dir, model, 5, 640, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7266b75-c9b6-4afd-817b-6b72244731a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  yolov5.models.yolo.Detect               [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n",
      "Scanning C:\\Users\\manhc\\yolov5\\data\\labels.cache... 809 images, 0 backgrounds, 0 corrupt: 100%|██████████| 809/809 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[65], line 11\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m get_image_and_label_files(data_yaml)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Huấn luyện và dự đoán\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrain_and_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[64], line 20\u001b[0m, in \u001b[0;36mtrain_and_predict\u001b[1;34m(train_dir, model, epoch, img_size, batch_size)\u001b[0m\n\u001b[0;32m     17\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Nếu chiều kênh là 1, loại bỏ chiều kênh\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Chuyển đổi tensor thành mảng numpy\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m img_np \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Chuyển đổi kích thước và định dạng kênh của ảnh\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# In ảnh ra\u001b[39;00m\n\u001b[0;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img_np)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9827ddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  yolov5.models.yolo.Detect               [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for image: data\\images\\1075391489.jpg\n",
      "Class: 27.0, Confidence: 0.6467801332473755, Bounding Box: (283.6567687988281, 147.92738342285156) - (397.9013366699219, 521.1416015625)\n",
      "Predictions for image: data\\images\\108232382.jpg\n",
      "Predictions for image: data\\images\\108232417.jpg\n",
      "Predictions for image: data\\images\\109950015.jpg\n",
      "Predictions for image: data\\images\\1107796028.jpg\n",
      "Class: 74.0, Confidence: 0.7064599990844727, Bounding Box: (190.0982666015625, 306.855712890625) - (448.72674560546875, 555.0388793945312)\n",
      "Predictions for image: data\\images\\1111800334.jpg\n",
      "Predictions for image: data\\images\\111389662.jpg\n",
      "Predictions for image: data\\images\\115519078.jpg\n",
      "Predictions for image: data\\images\\117933049.jpg\n",
      "Predictions for image: data\\images\\1201452086.jpg\n",
      "Predictions for image: data\\images\\1224626332.jpg\n",
      "Predictions for image: data\\images\\1224628248.jpg\n",
      "Predictions for image: data\\images\\1230939811.jpg\n",
      "Predictions for image: data\\images\\123937306.jpg\n",
      "Predictions for image: data\\images\\1251769192.jpg\n",
      "Predictions for image: data\\images\\1263617360.jpg\n",
      "Predictions for image: data\\images\\128500912.jpg\n",
      "Predictions for image: data\\images\\132888485.jpg\n",
      "Predictions for image: data\\images\\13455389.jpg\n",
      "Predictions for image: data\\images\\1352999361.jpg\n",
      "Predictions for image: data\\images\\1354674694.jpg\n",
      "Predictions for image: data\\images\\1358914296.jpg\n",
      "Class: 2.0, Confidence: 0.765293300151825, Bounding Box: (82.68631744384766, 149.5642547607422) - (287.4309387207031, 244.4832305908203)\n",
      "Class: 2.0, Confidence: 0.6823509335517883, Bounding Box: (573.626220703125, 152.99020385742188) - (639.307861328125, 235.64401245117188)\n",
      "Class: 2.0, Confidence: 0.5550957918167114, Bounding Box: (7.626983642578125, 127.42025756835938) - (609.727783203125, 622.19580078125)\n",
      "Predictions for image: data\\images\\1358914336.jpg\n",
      "Predictions for image: data\\images\\1359885883.jpg\n",
      "Class: 27.0, Confidence: 0.7028340697288513, Bounding Box: (307.41546630859375, 71.60673522949219) - (411.1962890625, 482.517333984375)\n",
      "Class: 27.0, Confidence: 0.6188339591026306, Bounding Box: (412.6368713378906, 83.16581726074219) - (496.8692932128906, 476.912109375)\n",
      "Predictions for image: data\\images\\1381841108.jpg\n",
      "Predictions for image: data\\images\\1391981336.jpg\n",
      "Predictions for image: data\\images\\1402065041.jpg\n",
      "Predictions for image: data\\images\\1408921955.jpg\n",
      "Predictions for image: data\\images\\1412494805.jpg\n",
      "Predictions for image: data\\images\\142479250.jpg\n",
      "Predictions for image: data\\images\\144488978.jpg\n",
      "Predictions for image: data\\images\\144503924.jpg\n",
      "Predictions for image: data\\images\\145074777.jpg\n",
      "Predictions for image: data\\images\\145116964.jpg\n",
      "Predictions for image: data\\images\\1484583303.jpg\n",
      "Predictions for image: data\\images\\1485104539.jpg\n",
      "Predictions for image: data\\images\\1492071424.jpg\n",
      "Predictions for image: data\\images\\1508285598.jpg\n",
      "Predictions for image: data\\images\\153237185.jpg\n",
      "Predictions for image: data\\images\\159166594.jpg\n",
      "Class: 2.0, Confidence: 0.6797579526901245, Bounding Box: (52.316192626953125, 309.5329895019531) - (163.07223510742188, 424.2475280761719)\n",
      "Class: 5.0, Confidence: 0.5267219543457031, Bounding Box: (0.30419158935546875, 87.93824768066406) - (53.08380889892578, 383.69134521484375)\n",
      "Predictions for image: data\\images\\159166596.jpg\n",
      "Predictions for image: data\\images\\1597577216.jpg\n",
      "Predictions for image: data\\images\\162002009.jpg\n",
      "Predictions for image: data\\images\\1639660678.jpg\n",
      "Predictions for image: data\\images\\165133120.jpg\n",
      "Predictions for image: data\\images\\165691999.jpg\n",
      "Predictions for image: data\\images\\1690276845.jpg\n",
      "Class: 32.0, Confidence: 0.7914893627166748, Bounding Box: (29.437772750854492, 115.32210540771484) - (43.0893440246582, 129.85086059570312)\n",
      "Predictions for image: data\\images\\170453766.jpg\n",
      "Predictions for image: data\\images\\170557401.jpg\n",
      "Predictions for image: data\\images\\1833817340.jpg\n",
      "Predictions for image: data\\images\\18526789.jpg\n",
      "Predictions for image: data\\images\\185712356.jpg\n",
      "Predictions for image: data\\images\\18581717.jpg\n",
      "Predictions for image: data\\images\\1888260223.jpg\n",
      "Predictions for image: data\\images\\18945268.jpg\n",
      "Predictions for image: data\\images\\189568643.jpg\n",
      "Predictions for image: data\\images\\1925162720.jpg\n",
      "Predictions for image: data\\images\\193274877.jpg\n",
      "Predictions for image: data\\images\\194328418.jpg\n",
      "Predictions for image: data\\images\\19731441.jpg\n",
      "Predictions for image: data\\images\\198509094.jpg\n",
      "Predictions for image: data\\images\\202435355.jpg\n",
      "Class: 74.0, Confidence: 0.5482481718063354, Bounding Box: (259.56207275390625, 210.37815856933594) - (391.02154541015625, 415.2386474609375)\n",
      "Predictions for image: data\\images\\2070616132.jpg\n",
      "Predictions for image: data\\images\\2073319900.jpg\n",
      "Predictions for image: data\\images\\2073826793.jpg\n",
      "Predictions for image: data\\images\\2080839955.jpg\n",
      "Predictions for image: data\\images\\208158931.jpg\n",
      "Predictions for image: data\\images\\2090094285.jpg\n",
      "Predictions for image: data\\images\\2111299790.jpg\n",
      "Predictions for image: data\\images\\2121332821.jpg\n",
      "Predictions for image: data\\images\\212161092.jpg\n",
      "Predictions for image: data\\images\\2126991906.jpg\n",
      "Predictions for image: data\\images\\2132909220.jpg\n",
      "Class: 27.0, Confidence: 0.6576939225196838, Bounding Box: (177.36376953125, 179.26254272460938) - (262.2806701660156, 415.9520568847656)\n",
      "Predictions for image: data\\images\\213783134.jpg\n",
      "Class: 68.0, Confidence: 0.730191707611084, Bounding Box: (221.81971740722656, 192.69259643554688) - (589.29052734375, 423.1431579589844)\n",
      "Predictions for image: data\\images\\2149071419.jpg\n",
      "Class: 27.0, Confidence: 0.5315362215042114, Bounding Box: (134.221923828125, 527.9893798828125) - (176.5133056640625, 639.1009521484375)\n",
      "Predictions for image: data\\images\\2160849129.jpg\n",
      "Predictions for image: data\\images\\2168708196.jpg\n",
      "Predictions for image: data\\images\\2169121853.jpg\n",
      "Predictions for image: data\\images\\217288720.jpg\n",
      "Predictions for image: data\\images\\2175802759.jpg\n",
      "Predictions for image: data\\images\\2175812747.jpg\n",
      "Predictions for image: data\\images\\2176584286.jpg\n",
      "Predictions for image: data\\images\\2180293872.jpg\n",
      "Predictions for image: data\\images\\2196352484.jpg\n",
      "Class: 74.0, Confidence: 0.6277129054069519, Bounding Box: (222.1939697265625, 8.1019287109375) - (560.699462890625, 349.7867126464844)\n",
      "Predictions for image: data\\images\\2201490255.jpg\n",
      "Predictions for image: data\\images\\2209126349.jpg\n",
      "Predictions for image: data\\images\\2219451836.jpg\n",
      "Predictions for image: data\\images\\2224243813.jpg\n",
      "Predictions for image: data\\images\\2228221904.jpg\n",
      "Predictions for image: data\\images\\2237130264.jpg\n",
      "Predictions for image: data\\images\\2244358347.jpg\n",
      "Predictions for image: data\\images\\2246298716.jpg\n",
      "Predictions for image: data\\images\\224886291.jpg\n",
      "Predictions for image: data\\images\\2256858637.jpg\n",
      "Predictions for image: data\\images\\2263927086.jpg\n",
      "Predictions for image: data\\images\\2270013615.jpg\n",
      "Class: 56.0, Confidence: 0.5003324747085571, Bounding Box: (457.89141845703125, 249.39146423339844) - (604.9927368164062, 479.22003173828125)\n",
      "Predictions for image: data\\images\\2274089022.jpg\n",
      "Predictions for image: data\\images\\2274303482.jpg\n",
      "Predictions for image: data\\images\\228153666.jpg\n",
      "Predictions for image: data\\images\\228464385.jpg\n",
      "Predictions for image: data\\images\\229746392.jpg\n",
      "Predictions for image: data\\images\\229840376.jpg\n",
      "Predictions for image: data\\images\\231655451.jpg\n",
      "Predictions for image: data\\images\\2317967173.jpg\n",
      "Predictions for image: data\\images\\2318775604.jpg\n",
      "Predictions for image: data\\images\\2329810624.jpg\n",
      "Predictions for image: data\\images\\2333110366.jpg\n",
      "Predictions for image: data\\images\\233507685.jpg\n",
      "Class: 27.0, Confidence: 0.5699681043624878, Bounding Box: (279.7749938964844, 236.98379516601562) - (330.2057189941406, 419.8868103027344)\n",
      "Class: 27.0, Confidence: 0.5642461776733398, Bounding Box: (325.8981018066406, 248.7005157470703) - (366.0314025878906, 425.66888427734375)\n",
      "Predictions for image: data\\images\\2348900650.jpg\n",
      "Predictions for image: data\\images\\2350426312.jpg\n",
      "Predictions for image: data\\images\\2350505168.jpg\n",
      "Predictions for image: data\\images\\2356560497.jpg\n",
      "Predictions for image: data\\images\\236248972.jpg\n",
      "Predictions for image: data\\images\\2364685038.jpg\n",
      "Predictions for image: data\\images\\2368653535.jpg\n",
      "Predictions for image: data\\images\\2383222548.jpg\n",
      "Predictions for image: data\\images\\239985057.jpg\n",
      "Predictions for image: data\\images\\2401584750.jpg\n",
      "Predictions for image: data\\images\\2403699729.jpg\n",
      "Predictions for image: data\\images\\2406598998.jpg\n",
      "Predictions for image: data\\images\\2407784266.jpg\n",
      "Predictions for image: data\\images\\2413159667.jpg\n",
      "Predictions for image: data\\images\\2416565238.jpg\n",
      "Predictions for image: data\\images\\2418409523.jpg\n",
      "Predictions for image: data\\images\\2421111243.jpg\n",
      "Predictions for image: data\\images\\2421405885.jpg\n",
      "Predictions for image: data\\images\\2421560364.jpg\n",
      "Class: 27.0, Confidence: 0.7218011617660522, Bounding Box: (167.05865478515625, 59.65010070800781) - (309.25628662109375, 510.31732177734375)\n",
      "Predictions for image: data\\images\\2422219264.jpg\n",
      "Predictions for image: data\\images\\2423654138.jpg\n",
      "Predictions for image: data\\images\\2423655766.jpg\n",
      "Predictions for image: data\\images\\2425891764.jpg\n",
      "Predictions for image: data\\images\\2427198460.jpg\n",
      "Predictions for image: data\\images\\2427200136.jpg\n",
      "Predictions for image: data\\images\\2435021305.jpg\n",
      "Predictions for image: data\\images\\2435838064.jpg\n",
      "Predictions for image: data\\images\\2439319086.jpg\n",
      "Predictions for image: data\\images\\2445338870.jpg\n",
      "Predictions for image: data\\images\\2445745857.jpg\n",
      "Predictions for image: data\\images\\2445982293.jpg\n",
      "Predictions for image: data\\images\\2450743885.jpg\n",
      "Predictions for image: data\\images\\2451569770.jpg\n",
      "Predictions for image: data\\images\\2452451692.jpg\n",
      "Predictions for image: data\\images\\2457001875.jpg\n",
      "Predictions for image: data\\images\\2459989055.jpg\n",
      "Predictions for image: data\\images\\2460201219.jpg\n",
      "Class: 27.0, Confidence: 0.5617940425872803, Bounding Box: (165.1952667236328, 84.53477478027344) - (269.6264953613281, 497.616943359375)\n",
      "Class: 27.0, Confidence: 0.5411783456802368, Bounding Box: (118.25055694580078, 192.41514587402344) - (248.89013671875, 504.795166015625)\n",
      "Predictions for image: data\\images\\2463217235.jpg\n",
      "Predictions for image: data\\images\\2463686290.jpg\n",
      "Predictions for image: data\\images\\246526320.jpg\n",
      "Predictions for image: data\\images\\2472817996.jpg\n",
      "Predictions for image: data\\images\\2476100758.jpg\n",
      "Predictions for image: data\\images\\2485974348.jpg\n",
      "Predictions for image: data\\images\\250858537.jpg\n",
      "Predictions for image: data\\images\\2514220918.jpg\n",
      "Predictions for image: data\\images\\2514607078.jpg\n",
      "Predictions for image: data\\images\\2515731280.jpg\n",
      "Predictions for image: data\\images\\2516793607.jpg\n",
      "Predictions for image: data\\images\\2518709496.jpg\n",
      "Class: 27.0, Confidence: 0.7064477205276489, Bounding Box: (40.373191833496094, 234.25856018066406) - (180.857666015625, 505.18115234375)\n",
      "Class: 27.0, Confidence: 0.5831447839736938, Bounding Box: (213.29641723632812, 112.12301635742188) - (323.1859436035156, 511.6610412597656)\n",
      "Class: 27.0, Confidence: 0.5424350500106812, Bounding Box: (113.15782165527344, 123.50425720214844) - (220.55970764160156, 495.66204833984375)\n",
      "Class: 27.0, Confidence: 0.539696216583252, Bounding Box: (236.42819213867188, 181.5069122314453) - (443.6903381347656, 509.23907470703125)\n",
      "Predictions for image: data\\images\\2519143928.jpg\n",
      "Predictions for image: data\\images\\2519368987.jpg\n",
      "Class: 9.0, Confidence: 0.6124280691146851, Bounding Box: (486.4390563964844, 39.39237976074219) - (504.0912170410156, 84.16068267822266)\n",
      "Predictions for image: data\\images\\2530090035.jpg\n",
      "Predictions for image: data\\images\\2531530868.jpg\n",
      "Predictions for image: data\\images\\2533950636.jpg\n",
      "Predictions for image: data\\images\\2534155497.jpg\n",
      "Predictions for image: data\\images\\2548848227.jpg\n",
      "Class: 9.0, Confidence: 0.79449862241745, Bounding Box: (566.341552734375, 457.6832580566406) - (582.12255859375, 499.6965026855469)\n",
      "Class: 2.0, Confidence: 0.7173858284950256, Bounding Box: (371.197021484375, 568.3251953125) - (636.9921875, 639.080078125)\n",
      "Predictions for image: data\\images\\2550056374.jpg\n",
      "Predictions for image: data\\images\\2553832700.jpg\n",
      "Predictions for image: data\\images\\255547402.jpg\n",
      "Predictions for image: data\\images\\255740214.jpg\n",
      "Predictions for image: data\\images\\2562313655.jpg\n",
      "Predictions for image: data\\images\\2568847460.jpg\n",
      "Predictions for image: data\\images\\2578628547.jpg\n",
      "Predictions for image: data\\images\\2583801040.jpg\n",
      "Predictions for image: data\\images\\2584787090.jpg\n",
      "Predictions for image: data\\images\\258770610.jpg\n",
      "Predictions for image: data\\images\\2587818591.jpg\n",
      "Predictions for image: data\\images\\260863506.jpg\n",
      "Predictions for image: data\\images\\2609822722.jpg\n",
      "Predictions for image: data\\images\\2615405816.jpg\n",
      "Class: 4.0, Confidence: 0.511773943901062, Bounding Box: (579.5744018554688, 502.7829895019531) - (599.6356811523438, 512.7415161132812)\n",
      "Predictions for image: data\\images\\2616906744.jpg\n",
      "Predictions for image: data\\images\\2618900486.jpg\n",
      "Predictions for image: data\\images\\2620571508.jpg\n",
      "Predictions for image: data\\images\\2646033238.jpg\n",
      "Predictions for image: data\\images\\2650126854.jpg\n",
      "Predictions for image: data\\images\\2650497443.jpg\n",
      "Predictions for image: data\\images\\2652633857.jpg\n",
      "Predictions for image: data\\images\\2654519236.jpg\n",
      "Predictions for image: data\\images\\2656155309.jpg\n",
      "Predictions for image: data\\images\\2656406197.jpg\n",
      "Predictions for image: data\\images\\2662264721.jpg\n",
      "Predictions for image: data\\images\\2674622648.jpg\n",
      "Predictions for image: data\\images\\2675240640.jpg\n",
      "Predictions for image: data\\images\\2675240646.jpg\n",
      "Class: 27.0, Confidence: 0.5473492741584778, Bounding Box: (387.96185302734375, 395.4521484375) - (452.02532958984375, 517.8133544921875)\n",
      "Predictions for image: data\\images\\2677577557.jpg\n",
      "Predictions for image: data\\images\\2680574577.jpg\n",
      "Predictions for image: data\\images\\2681480174.jpg\n",
      "Predictions for image: data\\images\\2698169582.jpg\n",
      "Predictions for image: data\\images\\2716412211.jpg\n",
      "Predictions for image: data\\images\\2725772074.jpg\n",
      "Predictions for image: data\\images\\2725852441.jpg\n",
      "Class: 77.0, Confidence: 0.7647001147270203, Bounding Box: (108.5081787109375, 537.396484375) - (212.28314208984375, 638.771728515625)\n",
      "Class: 9.0, Confidence: 0.7148907780647278, Bounding Box: (106.58123779296875, 216.54881286621094) - (208.8736572265625, 320.44964599609375)\n",
      "Class: 74.0, Confidence: 0.5225515961647034, Bounding Box: (3.181720733642578, 218.54611206054688) - (103.69879150390625, 319.7835998535156)\n",
      "Predictions for image: data\\images\\272645705.jpg\n",
      "Class: 76.0, Confidence: 0.6161255836486816, Bounding Box: (400.1849060058594, 274.04840087890625) - (493.3621520996094, 465.36590576171875)\n",
      "Predictions for image: data\\images\\2727066466.jpg\n",
      "Predictions for image: data\\images\\2729995962.jpg\n",
      "Predictions for image: data\\images\\2730165711.jpg\n",
      "Predictions for image: data\\images\\2760763703.jpg\n",
      "Predictions for image: data\\images\\2762358474.jpg\n",
      "Predictions for image: data\\images\\2766150734.jpg\n",
      "Predictions for image: data\\images\\2766852637.jpg\n",
      "Predictions for image: data\\images\\2767698530.jpg\n",
      "Predictions for image: data\\images\\2767935306.jpg\n",
      "Predictions for image: data\\images\\2774437111.jpg\n",
      "Predictions for image: data\\images\\2777121253.jpg\n",
      "Predictions for image: data\\images\\2777758833.jpg\n",
      "Predictions for image: data\\images\\2777979668.jpg\n",
      "Predictions for image: data\\images\\2780928833.jpg\n",
      "Predictions for image: data\\images\\2783736621.jpg\n",
      "Predictions for image: data\\images\\2786523391.jpg\n",
      "Class: 76.0, Confidence: 0.5839444994926453, Bounding Box: (317.51678466796875, 116.19757080078125) - (381.37750244140625, 319.9650573730469)\n",
      "Predictions for image: data\\images\\2789662807.jpg\n",
      "Predictions for image: data\\images\\2789664089.jpg\n",
      "Predictions for image: data\\images\\2790511610.jpg\n",
      "Predictions for image: data\\images\\2792076321.jpg\n",
      "Predictions for image: data\\images\\2792546855.jpg\n",
      "Class: 74.0, Confidence: 0.6926621198654175, Bounding Box: (215.24072265625, 183.80987548828125) - (444.06744384765625, 397.8529052734375)\n",
      "Predictions for image: data\\images\\2792685284.jpg\n",
      "Predictions for image: data\\images\\2794725337.jpg\n",
      "Class: 76.0, Confidence: 0.6089030504226685, Bounding Box: (209.2264404296875, 69.99891662597656) - (361.87451171875, 202.1003875732422)\n",
      "Predictions for image: data\\images\\2795207851.jpg\n",
      "Predictions for image: data\\images\\2795236586.jpg\n",
      "Predictions for image: data\\images\\2801899525.jpg\n",
      "Predictions for image: data\\images\\2809730445.jpg\n",
      "Predictions for image: data\\images\\281143123.jpg\n",
      "Predictions for image: data\\images\\281143147.jpg\n",
      "Predictions for image: data\\images\\2818828296.jpg\n",
      "Predictions for image: data\\images\\282208842.jpg\n",
      "Predictions for image: data\\images\\282628483.jpg\n",
      "Predictions for image: data\\images\\28271069.jpg\n",
      "Predictions for image: data\\images\\2838426683.jpg\n",
      "Predictions for image: data\\images\\2839448306.jpg\n",
      "Predictions for image: data\\images\\2844210941.jpg\n",
      "Predictions for image: data\\images\\284602830.jpg\n",
      "Predictions for image: data\\images\\2846084211.jpg\n",
      "Predictions for image: data\\images\\2855164257.jpg\n",
      "Predictions for image: data\\images\\2858462390.jpg\n",
      "Predictions for image: data\\images\\2865521932.jpg\n",
      "Predictions for image: data\\images\\2866536247.jpg\n",
      "Predictions for image: data\\images\\2867420595.jpg\n",
      "Predictions for image: data\\images\\2868736304.jpg\n",
      "Predictions for image: data\\images\\2874574766.jpg\n",
      "Predictions for image: data\\images\\2887641700.jpg\n",
      "Predictions for image: data\\images\\2891341625.jpg\n",
      "Predictions for image: data\\images\\2898435362.jpg\n",
      "Predictions for image: data\\images\\2900584649.jpg\n",
      "Predictions for image: data\\images\\2908611911.jpg\n",
      "Predictions for image: data\\images\\2913811697.jpg\n",
      "Class: 15.0, Confidence: 0.5029475092887878, Bounding Box: (3.2184600830078125, 467.35968017578125) - (299.0880126953125, 639.0637817382812)\n",
      "Predictions for image: data\\images\\2920059304.jpg\n",
      "Predictions for image: data\\images\\2920077270.jpg\n",
      "Predictions for image: data\\images\\2921061781.jpg\n",
      "Predictions for image: data\\images\\2926640633.jpg\n",
      "Predictions for image: data\\images\\2938026725.jpg\n",
      "Predictions for image: data\\images\\2938879364.jpg\n",
      "Predictions for image: data\\images\\2940596257.jpg\n",
      "Predictions for image: data\\images\\2941703909.jpg\n",
      "Predictions for image: data\\images\\2950778833.jpg\n",
      "Predictions for image: data\\images\\2958187326.jpg\n",
      "Predictions for image: data\\images\\2960027434.jpg\n",
      "Predictions for image: data\\images\\2962045.jpg\n",
      "Predictions for image: data\\images\\2974365916.jpg\n",
      "Predictions for image: data\\images\\2978106177.jpg\n",
      "Predictions for image: data\\images\\2982186650.jpg\n",
      "Predictions for image: data\\images\\2983583413.jpg\n",
      "Predictions for image: data\\images\\2988490694.jpg\n",
      "Predictions for image: data\\images\\2989750063.jpg\n",
      "Predictions for image: data\\images\\2990076322.jpg\n",
      "Predictions for image: data\\images\\2993064866.jpg\n",
      "Predictions for image: data\\images\\299768792.jpg\n",
      "Predictions for image: data\\images\\3002642406.jpg\n",
      "Predictions for image: data\\images\\3002734592.jpg\n",
      "Predictions for image: data\\images\\3005794070.jpg\n",
      "Predictions for image: data\\images\\3006946827.jpg\n",
      "Predictions for image: data\\images\\3007885565.jpg\n",
      "Predictions for image: data\\images\\3016882365.jpg\n",
      "Class: 9.0, Confidence: 0.5946822762489319, Bounding Box: (106.64205169677734, 195.594482421875) - (289.3396301269531, 434.43609619140625)\n",
      "Predictions for image: data\\images\\3023420658.jpg\n",
      "Predictions for image: data\\images\\3039312526.jpg\n",
      "Predictions for image: data\\images\\3046979862.jpg\n",
      "Predictions for image: data\\images\\3051999324.jpg\n",
      "Predictions for image: data\\images\\3054222264.jpg\n",
      "Predictions for image: data\\images\\3055058872.jpg\n",
      "Predictions for image: data\\images\\3065654531.jpg\n",
      "Predictions for image: data\\images\\3088544942.jpg\n",
      "Predictions for image: data\\images\\3088829946.jpg\n",
      "Predictions for image: data\\images\\3089967648.jpg\n",
      "Class: 74.0, Confidence: 0.5804985761642456, Bounding Box: (192.22000122070312, 339.9735107421875) - (457.7592468261719, 600.2974243164062)\n",
      "Predictions for image: data\\images\\3091292844.jpg\n",
      "Class: 61.0, Confidence: 0.6394047141075134, Bounding Box: (195.06678771972656, 200.36483764648438) - (248.2521514892578, 275.1661071777344)\n",
      "Predictions for image: data\\images\\3099884211.jpg\n",
      "Predictions for image: data\\images\\3101502490.jpg\n",
      "Predictions for image: data\\images\\3101679471.jpg\n",
      "Predictions for image: data\\images\\3103582592.jpg\n",
      "Class: 4.0, Confidence: 0.5163649320602417, Bounding Box: (339.84918212890625, 411.93695068359375) - (578.6466674804688, 514.2523803710938)\n",
      "Predictions for image: data\\images\\3103735966.jpg\n",
      "Class: 38.0, Confidence: 0.6909604668617249, Bounding Box: (171.2054443359375, 77.10595703125) - (328.77923583984375, 276.4659729003906)\n",
      "Predictions for image: data\\images\\310688144.jpg\n",
      "Class: 27.0, Confidence: 0.646881639957428, Bounding Box: (179.93841552734375, 180.68321228027344) - (261.58575439453125, 416.75494384765625)\n",
      "Class: 27.0, Confidence: 0.6349963545799255, Bounding Box: (374.241943359375, 180.61474609375) - (479.03863525390625, 566.21630859375)\n",
      "Predictions for image: data\\images\\3107573789.jpg\n",
      "Class: 27.0, Confidence: 0.5199823975563049, Bounding Box: (102.82484436035156, 139.13018798828125) - (375.76580810546875, 363.31689453125)\n",
      "Predictions for image: data\\images\\310764290.jpg\n",
      "Predictions for image: data\\images\\3109158407.jpg\n",
      "Predictions for image: data\\images\\3116688672.jpg\n",
      "Predictions for image: data\\images\\3117510822.jpg\n",
      "Predictions for image: data\\images\\3118428220.jpg\n",
      "Predictions for image: data\\images\\3119389772.jpg\n",
      "Predictions for image: data\\images\\3119389942.jpg\n",
      "Predictions for image: data\\images\\3125626922.jpg\n",
      "Predictions for image: data\\images\\3147491623.jpg\n",
      "Predictions for image: data\\images\\3152384527.jpg\n",
      "Predictions for image: data\\images\\3153222474.jpg\n",
      "Predictions for image: data\\images\\3153892834.jpg\n",
      "Predictions for image: data\\images\\3154099645.jpg\n",
      "Predictions for image: data\\images\\3161927635.jpg\n",
      "Predictions for image: data\\images\\3164722529.jpg\n",
      "Predictions for image: data\\images\\3164722903.jpg\n",
      "Predictions for image: data\\images\\3165551816.jpg\n",
      "Predictions for image: data\\images\\3165552688.jpg\n",
      "Predictions for image: data\\images\\3165553650.jpg\n",
      "Predictions for image: data\\images\\3168295809.jpg\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\b\\abs_bao0hdcrdh\\croot\\pytorch_1675190257512\\work\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1638400 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 82\u001b[0m\n\u001b[0;32m     80\u001b[0m img_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m640\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Gọi hàm train_by_yolov5\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[43mtrain_by_yolov5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 66\u001b[0m, in \u001b[0;36mtrain_by_yolov5\u001b[1;34m(data_dir, weights, config_path, img_size, conf_thres, iou_thres)\u001b[0m\n\u001b[0;32m     63\u001b[0m labels_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels_yolo)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Dự đoán\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Áp dụng non-maximum suppression và lọc kết quả\u001b[39;00m\n\u001b[0;32m     69\u001b[0m detections \u001b[38;5;241m=\u001b[39m non_max_suppression(outputs, conf_thres, iou_thres)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\yolov5\\yolov5\\models\\yolo.py:209\u001b[0m, in \u001b[0;36mDetectionModel.forward\u001b[1;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_augment(x)  \u001b[38;5;66;03m# augmented inference, None\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\yolov5\\yolov5\\models\\yolo.py:121\u001b[0m, in \u001b[0;36mBaseModel._forward_once\u001b[1;34m(self, x, profile, visualize)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 121\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    122\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\anaconda4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\yolov5\\yolov5\\models\\common.py:167\u001b[0m, in \u001b[0;36mC3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3(torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(x)), \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda4\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\yolov5\\yolov5\\models\\common.py:120\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\yolov5\\yolov5\\models\\common.py:56\u001b[0m, in \u001b[0;36mConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda4\\lib\\site-packages\\torch\\nn\\modules\\activation.py:391\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda4\\lib\\site-packages\\torch\\nn\\functional.py:2047\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2045\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   2046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 2047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\b\\abs_bao0hdcrdh\\croot\\pytorch_1675190257512\\work\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1638400 bytes."
     ]
    }
   ],
   "source": [
    "def train_by_yolov5(data_dir, weights=\"C:/Users/manhc/yolov5/yolov5/yolov5s.pt\", \n",
    "                    config_path=\"C:/Users/manhc/yolov5/yolov5/models/yolov5s.yaml\",\n",
    "                    img_size=640, conf_thres=0.5, iou_thres=0.5):\n",
    "    # Kiểm tra định dạng và kích thước ảnh đầu vào\n",
    "    check_img_size(img_size)\n",
    "\n",
    "    img_size = int(img_size)\n",
    "  \n",
    "    # Tạo mô hình YOLOv5\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = Model(config_path).to(device)\n",
    "    model.load_state_dict(torch.load(weights, map_location=device)['model'].float().state_dict())\n",
    "    model.eval()\n",
    "\n",
    "    # Kiểm tra đường dẫn đến thư mục chứa ảnh và nhãn\n",
    "    image_dir = os.path.join(data_dir, 'images')\n",
    "    label_dir = os.path.join(data_dir, 'labels')\n",
    "\n",
    "    # Lấy danh sách các file ảnh và nhãn\n",
    "    image_files = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.jpg')]\n",
    "    label_files = [os.path.join(label_dir, file) for file in os.listdir(label_dir) if file.endswith('.txt')]\n",
    "\n",
    "    # Kiểm tra số lượng file ảnh và nhãn\n",
    "    if len(image_files) == 0:\n",
    "        raise ValueError(\"No image files found in the specified directory.\")\n",
    "    if len(label_files) == 0:\n",
    "        raise ValueError(\"No label files found in the specified directory.\")\n",
    "    if len(image_files) != len(label_files):\n",
    "        raise ValueError(\"The number of image files and label files does not match.\")\n",
    "    \n",
    "    # Chuẩn bị ảnh và nhãn để đưa vào mô hình\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Huấn luyện và dự đoán\n",
    "    for img_file, label_file in zip(image_files, label_files):\n",
    "        # Đọc ảnh\n",
    "        img = Image.open(img_file).convert('RGB')\n",
    "\n",
    "        # Resize ảnh\n",
    "        img = img.resize((640, 640))\n",
    "        \n",
    "        # Đọc nhãn\n",
    "        with open(label_file, 'r') as f:\n",
    "            labels = f.read().strip().split('\\n')\n",
    "\n",
    "        # Chuyển đổi nhãn sang định dạng yolo\n",
    "        labels_yolo = []\n",
    "        for label in labels:\n",
    "            class_id, x, y, w, h = map(float, label.split())\n",
    "            x_center, y_center = x * img_size, y * img_size\n",
    "            width, height = w * img_size, h * img_size\n",
    "            x1 = int(x_center - width / 2)\n",
    "            y1 = int(y_center - height / 2)\n",
    "            x2 = int(x_center + width / 2)\n",
    "            y2 = int(y_center + height / 2)\n",
    "            labels_yolo.append([class_id, x1, y1, x2, y2])\n",
    "        \n",
    "        # Chuyển đổi ảnh và nhãn sang tensor và đưa vào device\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "        labels_tensor = torch.tensor(labels_yolo).unsqueeze(0).to(device)\n",
    "\n",
    "        # Dự đoán\n",
    "        outputs = model(img_tensor)\n",
    "        \n",
    "        # Áp dụng non-maximum suppression và lọc kết quả\n",
    "        detections = non_max_suppression(outputs, conf_thres, iou_thres)[0]\n",
    "        \n",
    "        # In kết quả\n",
    "        print(\"Predictions for image:\", img_file)\n",
    "        for x1, y1, x2, y2, conf, cls in detections:\n",
    "            print(f\"Class: {cls.item()}, Confidence: {conf.item()}, Bounding Box: ({x1.item()}, {y1.item()}) - ({x2.item()}, {y2.item()})\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Đường dẫn tới ảnh và nhãn\n",
    "    path = 'data'\n",
    "    img_size = 640\n",
    "    # Gọi hàm train_by_yolov5\n",
    "    train_by_yolov5(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264280d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
